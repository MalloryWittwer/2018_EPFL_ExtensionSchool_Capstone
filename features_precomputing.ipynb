{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @ Mallory Wittwer\n",
    "This notebook is used to pre-compute features that would otherwise take time to produce at training time. Pre-computed features are saved as NPY files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import piexif\n",
    "import piexif.helper\n",
    "import json\n",
    "import glob\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pymks import (\n",
    "    PrimitiveTransformer,\n",
    "    TwoPointCorrelation,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def read_metaImage(root):\n",
    "    '''Reads metadata embedded in a JPG file.'''\n",
    "    exif_dict = piexif.load(root)\n",
    "    user_comment = piexif.helper.UserComment.load(exif_dict[\"Exif\"][piexif.ExifIFD.UserComment])\n",
    "    d = json.loads(user_comment)\n",
    "    return d\n",
    "\n",
    "class MultiDataGenerator(tf.keras.utils.Sequence):\n",
    "    '''\n",
    "    Has an argument \"subset\" that enables to yield only a single texture class (for ex. \"Brick\").\n",
    "    '''\n",
    "    def __init__(self, root, batch_size=50, subset=None, res=(150,150)):\n",
    "        self.d = {'Brick':[0,1], 'Checker':[1,1], 'Magic':[2,2], 'Noise':[3,2], 'Wave':[4,2]}\n",
    "        self.root = root\n",
    "        self.bs = batch_size\n",
    "        self.rx, self.ry = res\n",
    "        self.files = glob.glob(root)\n",
    "        self.n = len(self.files)\n",
    "        if subset:\n",
    "            filt = np.array(self._get_class_names()) == subset\n",
    "            self.files = np.array(self.files)[filt]\n",
    "            self.n = len(self.files)\n",
    "        self.subset = subset\n",
    "        print(f'Initialized data generator. Found {self.n} files.')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.n/self.bs))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.files[idx*self.bs:(idx+1)*self.bs]\n",
    "        X, y = self._data_generation(batch)\n",
    "        return X, y\n",
    "    \n",
    "    def _data_generation(self, batch):\n",
    "        data = np.empty((len(batch), self.rx, self.ry, 3), dtype=np.uint8)\n",
    "        \n",
    "        if self.subset:\n",
    "            labels = np.empty((len(batch), self.d[self.subset][1]), dtype=np.float32)\n",
    "        else:\n",
    "            labels = np.empty(len(batch), dtype=np.uint8)\n",
    "        \n",
    "        for k, file in enumerate(batch):\n",
    "            data[k] = np.array(Image.open(file), dtype=np.uint8)\n",
    "            \n",
    "            if self.subset:       \n",
    "                labels[k] = list(read_metaImage(file).get('texture_params').values())[0]\n",
    "            else:\n",
    "                vals = list(read_metaImage(file).get('texture_params').keys())[0]\n",
    "                labels[k] = self.d[vals][0]\n",
    "                \n",
    "        return data, labels   \n",
    "    \n",
    "    def _get_class_names(self):\n",
    "        class_names = []\n",
    "        for file in self.files:\n",
    "            class_names.append(list(read_metaImage(file).get('texture_params').keys())[0])\n",
    "        return class_names\n",
    "    \n",
    "class TwoPtsCorrelator():\n",
    "    '''\n",
    "    Computes Two-points correlation feature maps.\n",
    "    '''\n",
    "    def __init__(self, ctf=150):\n",
    "        self.preprocessor = Pipeline(steps=[\n",
    "            (\"discritize\", PrimitiveTransformer(n_state=2, min_=0.0, max_=1.0)),\n",
    "            (\"correlations\", TwoPointCorrelation(periodic_boundary=True, cutoff=ctf, correlations=[(0, 1), (1, 1)])),\n",
    "        ])\n",
    "        \n",
    "    def transform(self, X):\n",
    "        gray_X = np.mean(X, axis=2) / 255.0\n",
    "        container = np.zeros((gray_X.shape[0], gray_X.shape[1], 2), dtype=np.float64)\n",
    "        twoPts = self.preprocessor.transform(gray_X).compute()[...,0] # Has two channels (channel 2 useless)\n",
    "        container[...,0] = gray_X\n",
    "        container[...,1][:twoPts.shape[0], :twoPts.shape[1]] = twoPts\n",
    "        return container\n",
    "    \n",
    "class ColorGenerator(MultiDataGenerator):\n",
    "    '''\n",
    "    Yields centroid colors of images via K-Means clustering.\n",
    "    '''\n",
    "    def __init__(self, root, batch_size=50, subset=None, res=(150,150)):\n",
    "        MultiDataGenerator.__init__(self, root, batch_size, subset=None, res=(150,150))\n",
    "        self.data_is_baked = False\n",
    "        self.rgbs_data = np.empty((self.n, 6), dtype=np.float32)\n",
    "    \n",
    "    def _data_generation(self, batch):\n",
    "        data = np.empty((len(batch), 6), dtype=np.float32)\n",
    "        labels = np.empty((len(batch), 6), dtype=np.float32)\n",
    "        for k, file in enumerate(batch):\n",
    "            if self.data_is_baked:\n",
    "                rgbs_data = self.rgbs_data[k*len(batch):(k+1)*len(batch)]\n",
    "            else:\n",
    "                rgbs_data = self._extract_rgbs(np.array(Image.open(file), dtype=np.uint8))\n",
    "                self.rgbs_data[k*len(batch):(k+1)*len(batch)] = rgbs_data\n",
    "            rgbs_labels = list(read_metaImage(file).get('material_params').values())[:-2]\n",
    "            if self.mse(rgbs_data[0:3], rgbs_labels[3:6]) < self.mse(rgbs_data[0:3], rgbs_labels[0:3]):\n",
    "                # Flip colors 1 and 2 (put k-means centroids in correct order based on mse distance)\n",
    "                temp = rgbs_data[0:3].copy()\n",
    "                rgbs_data[0:3] = rgbs_data[3:6]\n",
    "                rgbs_data[3:6] = temp # classic example... I forgot how to do this more efficiently\n",
    "            data[k] = rgbs_data\n",
    "            labels[k] = rgbs_labels\n",
    "        \n",
    "        if (k+1)*len(batch) == self.n:\n",
    "            self.data_is_baked = True\n",
    "        \n",
    "        return data[:,:3], labels[:,:3]\n",
    "    \n",
    "    def mse(self, a, b):\n",
    "        return np.mean(np.square(a-b))\n",
    "    \n",
    "    def _extract_rgbs(self, im):\n",
    "        '''Returns array of (6,) RGB colors based on K-Means clustering of an image'''\n",
    "        rx, ry, _ = im.shape\n",
    "        # Extract training data for K-Means\n",
    "        im_flat = im.copy().reshape((rx*ry, 3))\n",
    "        np.random.shuffle(im_flat)\n",
    "        data_extract = im_flat[:500]\n",
    "        # Fit KMeans\n",
    "        prime_colors = (KMeans(n_clusters=2).fit(data_extract).cluster_centers_ / 255.0).ravel()\n",
    "        return prime_colors\n",
    "    \n",
    "def precompute_correlations(gen, n_outputs=1, lim=None):\n",
    "    '''\n",
    "    lim - maximum number of batches to pre-compute\n",
    "    '''\n",
    "    correlator = TwoPtsCorrelator(ctf=gen.rx)\n",
    "    if lim is None:\n",
    "        lim = np.floor(gen.n / gen.bs).astype('int')\n",
    "    tot_X = np.empty((lim*gen.bs, gen.rx, gen.ry, 2), dtype=np.float32)\n",
    "    tot_y = np.empty((lim*gen.bs, n_outputs), dtype=np.float32)\n",
    "    bs = gen.bs\n",
    "    for k, (X, y) in enumerate(gen):\n",
    "        tot_X[(k*bs):(k+1)*bs] = [correlator.transform(x) for x in X]\n",
    "        tot_y[(k*bs):(k+1)*bs] = y\n",
    "        if (k+1) >= lim:\n",
    "            break\n",
    "    return tot_X, tot_y\n",
    "\n",
    "def precompute_colors(gen, lim=None):\n",
    "    '''\n",
    "    lim - maximum number of batches to pre-compute\n",
    "    '''\n",
    "    if lim is None:\n",
    "        lim = np.floor(gen.n / gen.bs).astype('int')\n",
    "    tot_X = np.empty((lim*gen.bs, 3), dtype=np.float32)\n",
    "    tot_y = np.empty((lim*gen.bs, 3), dtype=np.float32)\n",
    "    bs = gen.bs\n",
    "    for k, (X, y) in enumerate(gen):\n",
    "        tot_X[(k*bs):(k+1)*bs] = X\n",
    "        tot_y[(k*bs):(k+1)*bs] = y\n",
    "        if (k+1) >= lim:\n",
    "            break\n",
    "    return tot_X, tot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save pre-computed data\n",
    "training_root = os.path.abspath(os.path.join(os.getcwd(), f'renders/training/*.jpg'))\n",
    "validation_root = os.path.abspath(os.path.join(os.getcwd(), f'renders/validation/*.jpg'))\n",
    "test_root = os.path.abspath(os.path.join(os.getcwd(), f'renders/test/*.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-computing two-point correlations (for structural texture parameter estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized data generator. Found 2021 files.\n",
      "(200, 150, 150, 2)\n"
     ]
    }
   ],
   "source": [
    "# Texture names and number of parameters to predict\n",
    "subsets = {'Brick':1, 'Checker':1, 'Magic':2, 'Noise':2, 'Wave':2}\n",
    "\n",
    "for subset, out in subsets.items():\n",
    "    # Training set\n",
    "    xtr, ytr = precompute_correlations(\n",
    "        gen=MultiDataGenerator(training_root, subset=subset), n_outputs=out, lim=40)\n",
    "    np.save(os.path.abspath(os.path.join(os.getcwd(), f'point_correlations/{subset}/xtr.npy')), xtr)\n",
    "    np.save(os.path.abspath(os.path.join(os.getcwd(), f'point_correlations/{subset}/ytr.npy')), ytr)\n",
    "    \n",
    "    # Validation set\n",
    "    xval, yval = precompute_correlations(\n",
    "        gen=MultiDataGenerator(validation_root, batch_size=1, subset=subset), n_outputs=out)\n",
    "    np.save(os.path.abspath(os.path.join(os.getcwd(), f'point_correlations/{subset}/xval.npy')), xval)\n",
    "    np.save(os.path.abspath(os.path.join(os.getcwd(), f'point_correlations/{subset}/yval.npy')), yval)\n",
    "    \n",
    "    # Test set\n",
    "    xte, yte = precompute_correlations(\n",
    "        gen=MultiDataGenerator(test_root, batch_size=1, subset=subset), n_outputs=out)\n",
    "    np.save(os.path.abspath(os.path.join(os.getcwd(), f'point_correlations/{subset}/xte.npy')), xte)\n",
    "    np.save(os.path.abspath(os.path.join(os.getcwd(), f'point_correlations/{subset}/yte.npy')), yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-computing centroid colors by K-Means (for texture base color estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized data generator. Found 10200 files.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "xtr, ytr = precompute_colors(gen=ColorGenerator(training_root), lim=40)\n",
    "np.save(os.path.abspath(os.path.join(os.getcwd(), f'color_translation/xtr.npy')), xtr)\n",
    "np.save(os.path.abspath(os.path.join(os.getcwd(), f'color_translation/ytr.npy')), ytr)\n",
    "\n",
    "# Validation set\n",
    "xval, yval = precompute_colors(gen=ColorGenerator(validation_root))\n",
    "np.save(os.path.abspath(os.path.join(os.getcwd(), f'color_translation/xval.npy')), xval)\n",
    "np.save(os.path.abspath(os.path.join(os.getcwd(), f'color_translation/yval.npy')), yval)\n",
    "\n",
    "# Test set\n",
    "xte, yte = precompute_colors(gen=ColorGenerator(test_root))\n",
    "np.save(os.path.abspath(os.path.join(os.getcwd(), f'color_translation/xte.npy')), xte)\n",
    "np.save(os.path.abspath(os.path.join(os.getcwd(), f'color_translation/yte.npy')), yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
